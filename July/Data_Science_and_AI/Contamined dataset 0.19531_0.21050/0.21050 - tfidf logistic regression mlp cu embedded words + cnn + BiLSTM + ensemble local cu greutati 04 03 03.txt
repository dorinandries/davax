import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalAveragePooling1D, Bidirectional, LSTM, Dropout, Dense

# Log-loss
def compute_logloss(y_true, y_pred, eps=1e-15):
    p = np.clip(y_pred, eps, 1 - eps)
    N, _ = p.shape
    y1 = np.zeros_like(p); y1[np.arange(N), y_true] = 1
    return -np.sum(y1 * np.log(p)) / N

# Încarcare date și encoding etichete
df_train = pd.read_csv('dataset2.csv')
df_test  = pd.read_csv('test.csv')

X = df_train['text'].values
y = df_train['author'].map({'EAP':0,'HPL':1,'MWS':2}).values
X_test, ids_test = df_test['text'].values, df_test['id'].values

# Split local
X_tr, X_val, y_tr, y_val = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# --- TF-IDF + Logistic ---
tfidf = TfidfVectorizer(ngram_range=(1,2), max_features=50_000)
X_tr_tfidf  = tfidf.fit_transform(X_tr)
X_val_tfidf = tfidf.transform(X_val)
lr = LogisticRegression(max_iter=1_000)
lr.fit(X_tr_tfidf, y_tr)
pA_val = lr.predict_proba(X_val_tfidf)

# B
max_words, max_len = 20_000, 100
tok = Tokenizer(num_words=max_words);
tok.fit_on_texts(X_tr)
seq_tr = pad_sequences(tok.texts_to_sequences(X_tr), maxlen=max_len)
seq_val= pad_sequences(tok.texts_to_sequences(X_val), maxlen=max_len)

model_mlp = Sequential([
    Embedding(max_words, 100),
    GlobalAveragePooling1D(),
    Dropout(0.5),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax'),
])
model_mlp.compile(optimizer='adam', loss='sparse_categorical_crossentropy')
model_mlp.fit(seq_tr, y_tr, validation_data=(seq_val,y_val), epochs=5, batch_size=32)
pB_val = model_mlp.predict(seq_val)

# C
cnn = Sequential([
    Embedding(max_words, 100),
    Conv1D(64, 5, activation='relu'),
    # LSTM cu return_sequences=True ca să primească un tensor 3D
    Bidirectional(LSTM(32, return_sequences=True)),
    # După LSTM pool peste dimensiunea de timesteps
    GlobalAveragePooling1D(),
    Dropout(0.5),
    Dense(3, activation='softmax'),
])
cnn.compile('adam', 'sparse_categorical_crossentropy')
cnn.fit(seq_tr, y_tr, validation_data=(seq_val,y_val), epochs=5, batch_size=32)
pC_val = cnn.predict(seq_val)

# Ensemble A+B+C
# pABC_val = (pA_val + pB_val + pC_val) / 3
pABC_val = 0.4 * pA_val + 0.3 * pB_val + 0.3 * pC_val

print("3rd try - Log-loss local:", compute_logloss(y_val, pABC_val))

# Reantrenare finală pe tot și predicții test
lr.fit(tfidf.fit_transform(X), y)
model_mlp.fit(pad_sequences(tok.texts_to_sequences(X), maxlen=max_len), y, epochs=3, batch_size=32)
cnn.fit(pad_sequences(tok.texts_to_sequences(X), maxlen=max_len), y, epochs=3, batch_size=32)

pA_test = lr.predict_proba(tfidf.transform(X_test))
pB_test = model_mlp.predict(pad_sequences(tok.texts_to_sequences(X_test), maxlen=max_len))
pC_test = cnn.predict(pad_sequences(tok.texts_to_sequences(X_test), maxlen=max_len))
# pABC_test = (pA_test + pB_test + pC_test) / 3
pABC_test = 0.4 * pA_test + 0.3 * pB_test + 0.3 * pC_test

sub = pd.DataFrame(pABC_test, columns=['EAP','HPL','MWS'])
sub.insert(0, 'id', ids_test)
sub.to_csv('dataset2_10%_submission_ABC.csv', index=False)
